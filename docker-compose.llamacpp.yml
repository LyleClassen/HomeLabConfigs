services:
  llama-cpp:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-cpp
    restart: unless-stopped
    ports:
      - "8001:8080"
    volumes:
      # Host folder where you store GGUF models
      - /opt/llama/models:/models
    # Command-line args for llama-server
    # Optimized for: AMD RX 580 (4GB VRAM), i7-6700 (8 threads), 32GB RAM
    command:
      - -m
      - /models/llama-3.2-1b-instruct-Q4_K_M.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --n-gpu-layers
      - "99"           # Offload all layers to GPU (1B model fits in 4GB VRAM)
      - --threads
      - "6"            # Leave 2 threads for system/other containers
      - --ctx-size
      - "4096"         # Good context window, uses ~2GB VRAM
      - --api-key
      - "dummy"        # Change this to a real key for production!
    devices:
      # GPU access for Vulkan
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-llama
    restart: unless-stopped
    ports:
      - "3007:8080"
    volumes:
      - /opt/openwebui-llama/data:/app/backend/data
    environment:
      # Wire Open WebUI to llama.cpp's OpenAI-style API
      OPENAI_API_BASE_URL: http://llama-cpp:8080/v1
      OPENAI_API_KEY: dummy
      WEBUI_NAME: Home Lab Llama
      ENABLE_SIGNUP: "true"
    depends_on:
      - llama-cpp
    networks:
      - llama-net

networks:
  llama-net:
    driver: bridge
    name: llama-network