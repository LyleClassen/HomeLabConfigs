services:
# ------------------------------------------------
# OpenAI-style router – single entry-point
# ------------------------------------------------
  llama-router:
    image: ghcr.io/answerai/openai-router:latest   # 5 MB, arm64 & amd64
    container_name: llama-router
    restart: unless-stopped
    ports:
      - "7999:8000"          # ← your single API port
    environment:
      # map “model” name → upstream base URL
      OPENAI_ROUTER_MODELS: >
        {
          "llama-3.2-1b-instruct": "http://llama-3.2-1b-instruct:8080/v1",
          "embeddinggemma-300m":   "http://embeddinggemma-300m:8080/v1",
          "nomic-embed-v1.5":    "http://nomic-embed-v1.5:8080/v1",
          "qwen3-embedding-4b":  "http://qwen3-embedding-4b:8080/v1"
        }
      OPENAI_ROUTER_DEFAULT_MODEL: "llama-3.2-1b-instruct"
    depends_on:
      - llama-3.2-1b-instruct
      - embeddinggemma-300m
      - nomic-embed-v1.5
      - qwen3-embedding-4b
    networks:
      - llama-net
  # ============================================
  # Chat/Instruct Model - Llama 3.2 1B
  # ============================================
  llama-3.2-1b-instruct:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-3.2-1b-instruct
    restart: unless-stopped
    ports:
      - "8001:8080"
    volumes:
      - /opt/llama/models:/models
    # Optimized for: AMD RX 580 (4GB VRAM), i7-6700 (8 threads), 32GB RAM
    command:
      - -m
      - /models/llama-3.2-1b-instruct-Q4_K_M.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --n-gpu-layers
      - "99"
      - --threads
      - "6"
      - --ctx-size
      - "4096"
      - --api-key
      - "dummy"
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  # ============================================
  # Embedding Model - Gemma 300M
  # ============================================
  embeddinggemma-300m:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: embeddinggemma-300m
    restart: unless-stopped
    ports:
      - "8002:8080"
    volumes:
      - /opt/llama/models:/models
    command:
      - -m
      - /models/embeddinggemma-300M-F32.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --embeddings
      - --n-gpu-layers
      - "99"
      - --threads
      - "4"
      - --ctx-size
      - "2048"
      - --api-key
      - "dummy"
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  # ============================================
  # Embedding Model - Nomic Embed Text v1.5
  # ============================================
  nomic-embed-v1.5:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: nomic-embed-v1.5
    restart: unless-stopped
    ports:
      - "8003:8080"
    volumes:
      - /opt/llama/models:/models
    command:
      - -m
      - /models/nomic-embed-text-v1.5.f32.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --embeddings
      - --n-gpu-layers
      - "99"
      - --threads
      - "4"
      - --ctx-size
      - "8192"
      - --api-key
      - "dummy"
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  # ============================================
  # Embedding Model - Qwen3 Embedding 4B
  # ============================================
  qwen3-embedding-4b:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: qwen3-embedding-4b
    restart: unless-stopped
    ports:
      - "8004:8080"
    volumes:
      - /opt/llama/models:/models
    command:
      - -m
      - /models/Qwen3-Embedding-4B-Q4_K_M.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --embeddings
      - --n-gpu-layers
      - "99"
      - --threads
      - "4"
      - --ctx-size
      - "8192"
      - --api-key
      - "dummy"
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-llama
    restart: unless-stopped
    ports:
      - "3007:8080"
    volumes:
      - /opt/openwebui-llama/data:/app/backend/data
    environment:
      # Wire Open WebUI to llama.cpp's OpenAI-style API
      OPENAI_API_BASE_URL: http://llama-3.2-1b-instruct:8080/v1
      OPENAI_API_KEY: dummy
      WEBUI_NAME: Home Lab Llama
      ENABLE_SIGNUP: "true"
    depends_on:
      - llama-3.2-1b-instruct
    networks:
      - llama-net

networks:
  llama-net:
    driver: bridge
    name: llama-network