services:
  # ------------------------------------------------
  # OpenAI-style router â€“ single entry-point
  # ------------------------------------------------
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    container_name: litellm-router
    restart: unless-stopped
    ports:
      - "7999:4000"
    environment:
      LITELLM_CONFIG_YAML: |
        model_list:
          - model_name: llama-3.2-1b-instruct
            litellm_params:
              model: openai/llama-3.2-1b-instruct
              api_base: http://llama-3.2-1b-instruct:8080/v1
              api_key: dummy

          - model_name: embeddinggemma-300m
            litellm_params:
              model: openai/embeddinggemma-300m
              api_base: http://embeddinggemma-300m:8080/v1
              api_key: dummy

          - model_name: nomic-embed-v1.5
            litellm_params:
              model: openai/nomic-embed-v1.5
              api_base: http://nomic-embed-v1.5:8080/v1
              api_key: dummy

          - model_name: qwen3-embedding-4b
            litellm_params:
              model: openai/qwen3-embedding-4b
              api_base: http://qwen3-embedding-4b:8080/v1
              api_key: dummy
    entrypoint: ["/bin/sh", "-lc"]
    command:
      - |
        rm -rf /app/config.yaml
        printf "%s" "$LITELLM_CONFIG_YAML" > /app/config.yaml
        exec litellm --port 4000 --config /app/config.yaml
    depends_on:
      - llama-3.2-1b-instruct
      - embeddinggemma-300m
      - nomic-embed-v1.5
      - qwen3-embedding-4b
    networks:
      - llama-net
  # ============================================
  # Chat/Instruct Model - Llama 3.2 1B
  # ============================================
  llama-3.2-1b-instruct:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-3.2-1b-instruct
    restart: unless-stopped
    ports:
      - "8001:8080"
    volumes:
      - /opt/llama/models:/models
    # Optimized for: AMD RX 580 (4GB VRAM), i7-6700 (8 threads), 32GB RAM
    command:
      - -m
      - /models/llama-3.2-1b-instruct-Q4_K_M.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --n-gpu-layers
      - "99"
      - --threads
      - "6"
      - --ctx-size
      - "4096"
      - --api-key
      - "dummy"
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  # ============================================
  # Embedding Model - Gemma 300M
  # ============================================
  embeddinggemma-300m:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: embeddinggemma-300m
    restart: unless-stopped
    ports:
      - "8002:8080"
    volumes:
      - /opt/llama/models:/models
    command:
      - -m
      - /models/embeddinggemma-300M-F32.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --embeddings
      - --n-gpu-layers
      - "99"
      - --threads
      - "4"
      - --ctx-size
      - "2048"
      - --api-key
      - "dummy"
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  # ============================================
  # Embedding Model - Nomic Embed Text v1.5
  # ============================================
  nomic-embed-v1.5:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: nomic-embed-v1.5
    restart: unless-stopped
    ports:
      - "8003:8080"
    volumes:
      - /opt/llama/models:/models
    command:
      - -m
      - /models/nomic-embed-text-v1.5.f32.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --embeddings
      - --n-gpu-layers
      - "99"
      - --threads
      - "4"
      - --ctx-size
      - "8192"
      - --api-key
      - "dummy"
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  # ============================================
  # Embedding Model - Qwen3 Embedding 4B
  # ============================================
  qwen3-embedding-4b:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: qwen3-embedding-4b
    restart: unless-stopped
    ports:
      - "8004:8080"
    volumes:
      - /opt/llama/models:/models
    command:
      - -m
      - /models/Qwen3-Embedding-4B-Q4_K_M.gguf
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --embeddings
      - --n-gpu-layers
      - "99"
      - --threads
      - "4"
      - --ctx-size
      - "8192"
      - --api-key
      - "dummy"
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    networks:
      - llama-net

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-llama
    restart: unless-stopped
    ports:
      - "3007:8080"
    volumes:
      - /opt/openwebui-llama/data:/app/backend/data
    environment:
      # Wire Open WebUI to llama-router for access to all models
      OPENAI_API_BASE_URL: http://litellm:4000/v1
      OPENAI_API_KEY: dummy
      WEBUI_NAME: Home Lab Llama
      ENABLE_SIGNUP: "true"
    depends_on:
      - litellm
    networks:
      - llama-net

networks:
  llama-net:
    driver: bridge
    name: llama-network